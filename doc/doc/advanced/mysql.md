---
layout: default
---

# Using DeepDive with MySQL / MySQL Cluster

This document describes how to use DeepDive with
[MySQL](http://www.mysql.com/) and 
[MySQL Cluster](http://www.mysql.com/products/cluster/).

We still encourage developers to use PostgreSQL or
[GreenPlum](greenplum.html) since it gives maximum functionality of
DeepDive. However, DeepDive should also work well with MySQL or MySQL
cluster, other than caveats below.

### Common Caveats

Extractor types: only [tsv_extractor](../basics/extractors.html#tsv_extractor), 
[sql_extractor](../basics/extractors.html#sql_extractor) 
and [cmd_extractor](../basics/extractors.html#cmd_extractor) are supported.

When using TSV extractor with MySQL, make sure you are aware of the following caveats:

- NULL columns in extractor input
  are **string `NULL`**, which has different behavior with Postgres (input
  to extractor is `\N`). If you want to output NULL values in extractors
  in MySQL, you still need to output `\N`.

- Boolean fields in extractor input are `0` or `1`, which has different 
  behavior with Postgres (input
  to extractor is `true` / `false`). If you want to output boolean values in extractors
  in MySQL, you also need to output `0` / `1`.

If you are porting your applications from PostgreSQL to MySQL, be sure
your SQL queries are optimal, since some queries optimized for
PostgreSQL may be not optimal in MySQL. For example, you may need to
create more indexes to speed up queries with joins, depending on your
MySQL version.

### Caveats for MySQL Cluster

When configuring your MySQL cluster, be sure to follow available best practices online such as [this page](https://blogs.oracle.com/MySQL/entry/mysql_cluster_performance_best_practices). 

If you are using MySQL cluster (with "ndb" or "ndbcluster" engine), you may want to use the 
[parallel data loader](#ndbloader) discussed below, to speed up the data 
loading process.


For MySQL cluster, tables need to be partitioned since "Partitioning by KEY (including LINEAR KEY) is the only type of partitioning supported for the NDB storage engine". (http://dev.mysql.com/doc/refman/5.6/en/partitioning-limitations-storage-engines.html)

You need to distribute tables by a key which are not text/blob.

### Parallel data loader for MySQL Cluster

Data loading for MySQL Cluster can be slow using SQL interfaces. MySQL Cluster has provided [NDB API](http://dev.mysql.com/doc/ndbapi/en/) for faster access to cluster data. We provide a data loader in `DEEPDIVE_HOME/util/ndbloader/` that can be used to load data files in TSV format into MySQL cluster.

We integrate this parallel loader into data loading in our [extractors](../basics/extractors.html). 
To use this loader, specify `loader: ndbloader` and a `loader_config` in TSV extractors in `application.conf` then, data generated by extractors will be loaded into database using `ndbloader`. 

#### Configuration

When using ndbloader, make sure to include the library you need. On mac, use:

    DYLD_LIBRARY_PATH=`mysql_config --variable=pkglibdir`:$DYLD_LIBRARY_PATH

On Linux, use:

    LD_LIBRARY_PATH=`mysql_config --variable=pkglibdir`:$LD_LIBRARY_PATH

We provided preshipped binaries in `DEEPDIVE_HOME/util/ndbloader/ndbloader-mac` and `DEEPDIVE_HOME/util/ndbloader/ndbloader-linux`, and DeepDive will choose which to use based on detected OS. The preshipped binaries are compiled with MySQL cluster version `5.6.19-ndb-7.3.6-cluster-gpl`. 
If you are not using this version, make sure to compile your own using `./build.sh` under `DEEPDIVE_HOME/util/ndbloader/`, and replace the corresponding `ndbloader-mac` or `ndbloader-linux` with your compiled binary.

### How to use it

Currently our `ndbloader` can only load data in TSV format, without line breaks or `\t` characters in data.

Here is an example of using the parallel loader for MySQL cluster (it can be found in `DEEPDIVE/examples/spouse_example/tsv_extractor/application_mysqlcluster.conf`):

```bash
    ext_people {
      input: """
          SELECT  sentence_id, 
                  words,
                  ner_tags
          FROM    sentences
          """
      output_relation: "people_mentions"
      udf: ${APP_HOME}"/udf/ext_people.py"
      dependencies: ["ext_create_index_sentences"]
      input_batch_size: 4000
      style: "tsv_extractor"

      loader: "ndbloader"   # Using the parallel loader
      loader_config: {      # Specify a loader config
        # A loader schema file that specifies table schema to load into (mandatory)
        schema: ${APP_HOME}"/mysqlcluster/people_mentions.loaderschema"
        # Connection string to NDB cluster management node (mandatory)
        connection: "127.0.0.1:1186"
        # Number of threads that do parallel writeback, default 1
        threads: 4
        # Number of parallel transactions for each thread, default 60
        parallel_transactions: 100
      }
    }
```

- `connection` is the connection string to NDB cluster management node.

- `threads` is the number of threads that do parallel writeback, default 1. More threads can achieve more parallelism. Choose this value according to database configuration as well as number of cores in the computer.

- `parallel_transactions` is the number of parallel transactions for each thread, default 60. Increase your MySQL cluster maximum support of concurrent threads in order to achieve a bigger number here. Note that `parallel_transactions`x`threads` should not go over `MaxNoOfConcurrentTransactions`.

- For `schema` option, see below.

#### Loader schema file format

`schema` points to a file that specifies schema of output relation that loader will load data into. The file format is:

```
tableName [nokey]
column1 type1
column2 type2
...
columnN typeN
```

All fields are separated by spaces.

In the first line, specify table name, and optionally a `nokey` keyword to indicate that the table do not have a primary key on it. If the table has a primary key, do not include `nokey`; otherwise `nokey` must be included.

In following lines, each line specifies a column name and a column type, separated by a space. Columns must have the same order with the TSV file. Types must be one of below:

- `int`: INT/BIGINT, or equivalent types
- `real`: FLOAT/DOUBLE, or equivalent types
- `text`: TEXT/BLOB type
- `varchar`: VARCHAR type
- `char`: CHAR type
- `boolean`: BOOLEAN type

Note that only strictly `int`, `real`, `text`, `varchar`, `char`, `boolean` are supported in the schema file, rather than their equivalent types in MySQL.

Note that **Extra spaces, extra lines, or comments are not allowed** in the schema file.

#### Example schema files

Some SQL tables and their corresponding example schema files are like: (can be found in `DEEPDIVE/examples/spouse_example/tsv_extractor/mysqlcluster/`)

Example 1:

```sql
CREATE TABLE people_mentions(
  sentence_id varchar(255),
  start_position int,
  length int,
  text varchar(255),
  mention_id varchar(255),  -- unique identifier for people_mentions
  PRIMARY KEY(mention_id)
  ) engine=ndb;
```

Loader schema file:

```bash
people_mentions
sentence_id varchar
start_position int
length int
text varchar
mention_id varchar
```

Example 2:

```sql
CREATE TABLE has_spouse(
  person1_id varchar(255),
  person2_id varchar(255),
  sentence_id varchar(255),
  description text,
  is_true boolean,
  relation_id varchar(255), -- unique identifier for has_spouse
  id bigint                 -- reserved for DeepDive
  , PRIMARY KEY (person1_id, person2_id) -- this works for ndb for better partition
  ) engine=ndb PARTITION BY KEY (person1_id);
```

Loader schema file:

```bash
has_spouse
person1_id varchar
person2_id varchar
sentence_id varchar
description text
is_true boolean
relation_id varchar
id bigint
```

Example 3:

```sql
CREATE TABLE has_spouse_features(
  relation_id varchar(255),
  feature text
  ) engine=ndb;
```

Loader schema file:

```bash
has_spouse_features nokey
relation_id varchar
feature text
```


#### Caveats for ndbloader

- When using BLOB/TEXT types and primary keys, **make sure primary key columns are before any BLOB/TEXT column**, or you will experience "Invalid usage of blob attribute" errors


- `\N` will be loaded as `NULL`, which are coherent with other loading strategy in DeepDive. Make sure your tsv extractor UDF outputs `\N` for null values.



### <a name="faq" href="#"></a> FAQs

- **When using the data loader with MySQL Cluster, I got errors 
"Time-out in NDB, probably caused by deadlock".**

  Consider turning the `TransactionDeadlockDetectionTimeout` into a larger value (e.g. 60000) in your MySQL Cluster configuration.

- **In grounding phase, I got "ERROR 1297 (HY000): Got temporary error 
233 ': Out of operation records in transaction coordinator (increase 
MaxNoOfConcurrentOperations)"**

  Try to increase `MaxNoOfConcurrentOperations` and `MaxNoOfConcurrentTransactions` in your MySQL Cluster configuration.

